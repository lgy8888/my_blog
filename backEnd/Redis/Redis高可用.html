<!doctype html>
<html lang="zh-CN" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.0" />
    <meta name="theme" content="VuePress Theme Hope 2.0.0-rc.2" />
    <style>
      html {
        background: var(--bg-color, #fff);
      }

      html[data-theme="dark"] {
        background: var(--bg-color, #1d1e1f);
      }

      body {
        background: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.documentElement.setAttribute("data-theme", "dark");
      }
    </script>
    <meta property="og:url" content="https://vuepress-theme-hope-docs-demo.netlify.app/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html"><meta property="og:site_name" content="林光远的个人笔记"><meta property="og:title" content="Redis高可用"><meta property="og:description" content="概念 持久化是保存快照或日志记录在磁盘上，等待重启时恢复。但是如果机器故障了或者服务器宕机了，数据就会全没了 为了避免单点故障通常的做法是将数据库复制多个副本以部署在不同的服务器上 这样即使有一台服务器出现故障，其他服务器依然可以继续提供服务。 Redis 提供了复制（replication）功能，可以实现当一台数据库中的数据更新后，自动将更新的数据..."><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="article:author" content="LGYNB"><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Redis高可用","image":[""],"dateModified":null,"author":[{"@type":"Person","name":"LGYNB","url":"/"}]}</script><link rel="icon" href="/my_blog/favicon.ico"><title>Redis高可用 | 林光远的个人笔记</title><meta name="description" content="概念 持久化是保存快照或日志记录在磁盘上，等待重启时恢复。但是如果机器故障了或者服务器宕机了，数据就会全没了 为了避免单点故障通常的做法是将数据库复制多个副本以部署在不同的服务器上 这样即使有一台服务器出现故障，其他服务器依然可以继续提供服务。 Redis 提供了复制（replication）功能，可以实现当一台数据库中的数据更新后，自动将更新的数据...">
    <link rel="preload" href="/my_blog/assets/style-ERUp4lUP.css" as="style"><link rel="stylesheet" href="/my_blog/assets/style-ERUp4lUP.css">
    <link rel="modulepreload" href="/my_blog/assets/app-OTaO6_y0.js"><link rel="modulepreload" href="/my_blog/assets/Redis高可用.html-BSyHFJoo.js"><link rel="modulepreload" href="/my_blog/assets/Redis高可用.html-No9fLBKO.js"><link rel="modulepreload" href="/my_blog/assets/plugin-vue_export-helper-x3n3nnut.js">
    
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="vp-skip-link sr-only">跳至主要內容</a><!--]--><div class="theme-container has-toc"><!--[--><header id="navbar" class="vp-navbar"><div class="vp-navbar-start"><button type="button" class="vp-toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!--[--><!----><!--]--><!--[--><a class="vp-link vp-brand vp-brand" href="/my_blog/"><img class="vp-nav-logo" src="/my_blog/logo.png" alt="林光远的个人笔记"><!----><span class="vp-site-name hide-in-pad">林光远的个人笔记</span></a><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-center"><!--[--><!----><!--]--><!--[--><nav class="vp-nav-links"><div class="nav-item hide-in-mobile"><a aria-label="后端技术" class="vp-link nav-link active nav-link active" href="/my_blog/backEnd/"><span class="font-icon icon iconfont icon-back-stage" style=""></span>后端技术<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="Web开发" class="vp-link nav-link nav-link" href="/my_blog/webDevelop/"><span class="font-icon icon iconfont icon-config" style=""></span>Web开发<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="AI技术" class="vp-link nav-link nav-link" href="/my_blog/ai/"><span class="font-icon icon iconfont icon-function" style=""></span>AI技术<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="数据结构与算法" class="vp-link nav-link nav-link" href="/my_blog/coding/"><span class="font-icon icon iconfont icon-calculate" style=""></span>数据结构与算法<!----></a></div><div class="nav-item hide-in-mobile"><a aria-label="项目经历" class="vp-link nav-link nav-link" href="/my_blog/projects/"><span class="font-icon icon iconfont icon-code" style=""></span>项目经历<!----></a></div></nav><!--]--><!--[--><!----><!--]--></div><div class="vp-navbar-end"><!--[--><!----><!--]--><!--[--><!----><div class="nav-item vp-repo"><a class="vp-repo-link" href="https://github.com/lgy8888/my_blog.git" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button type="button" id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" placeholder="搜索本站" autocomplete="off" spellcheck="false" value><!----></form><!--]--><!--[--><!----><!--]--><button type="button" class="vp-toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span><span class="vp-top"></span><span class="vp-middle"></span><span class="vp-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow start"></span></div><aside id="sidebar" class="vp-sidebar"><!--[--><!----><!--]--><ul class="vp-sidebar-links"><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable active" type="button"><span class="font-icon icon iconfont icon-cache" style=""></span><span class="vp-sidebar-title">Redis</span><span class="vp-arrow down"></span></button><ul class="vp-sidebar-links"><li><!--[--><a aria-label="分布式系统理论" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA.html"><span class="font-icon icon iconfont icon-write" style=""></span>分布式系统理论<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis数据结构" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis数据结构<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis数据结构面试题汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis数据结构面试题汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis的运作" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E7%9A%84%E8%BF%90%E4%BD%9C.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis的运作<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis的运作面试题汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E7%9A%84%E8%BF%90%E4%BD%9C%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis的运作面试题汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis与客户端" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E4%B8%8E%E5%AE%A2%E6%88%B7%E7%AB%AF.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis与客户端<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis持久化" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis持久化<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis持久化面试题汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis持久化面试题汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis高可用" class="vp-link nav-link active vp-sidebar-link vp-sidebar-page active nav-link active vp-sidebar-link vp-sidebar-page active" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis高可用<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="概念" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#概念"><!---->概念<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="主从模式" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#主从模式"><!---->主从模式<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="确定主从服务器" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#确定主从服务器"><!---->确定主从服务器<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="第一次同步" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#第一次同步"><!---->第一次同步<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="命令传播" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#命令传播"><!---->命令传播<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="主服务器的压力分摊" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#主服务器的压力分摊"><!---->主服务器的压力分摊<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="增量复制" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#增量复制"><!---->增量复制<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="主从模式常见问题" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#主从模式常见问题"><!---->主从模式常见问题<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="哨兵机制" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#哨兵机制"><!---->哨兵机制<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="哨兵机制工作流程" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#哨兵机制工作流程"><!---->哨兵机制工作流程<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="判断主节点是否故障" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#判断主节点是否故障"><!---->判断主节点是否故障<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="主从故障转移" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#主从故障转移"><!---->主从故障转移<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="哨兵集群" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#哨兵集群"><!---->哨兵集群<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li><li class="vp-sidebar-sub-header"><a aria-label="分片集群" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#分片集群"><!---->分片集群<!----></a><ul class="vp-sidebar-sub-headers"><li class="vp-sidebar-sub-header"><a aria-label="分片集群特征" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#分片集群特征"><!---->分片集群特征<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="分片集群的可扩展" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#分片集群的可扩展"><!---->分片集群的可扩展<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="集群的组合方式" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#集群的组合方式"><!---->集群的组合方式<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="集群数据的分片原理" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#集群数据的分片原理"><!---->集群数据的分片原理<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="client访问数据集群的过程" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#client访问数据集群的过程"><!---->client访问数据集群的过程<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="数据复制过程和故障转移" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#数据复制过程和故障转移"><!---->数据复制过程和故障转移<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="分片集群的架构" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#分片集群的架构"><!---->分片集群的架构<!----></a><ul class="vp-sidebar-sub-headers"></ul></li><li class="vp-sidebar-sub-header"><a aria-label="RedisCluster构建缓存集群的最佳实践" class="vp-link nav-link vp-sidebar-link vp-heading nav-link vp-sidebar-link vp-heading" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8.html#rediscluster构建缓存集群的最佳实践"><!---->RedisCluster构建缓存集群的最佳实践<!----></a><ul class="vp-sidebar-sub-headers"></ul></li></ul></li></ul><!--]--></li><li><!--[--><a aria-label="Redis高可用面试题汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis高可用面试题汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis应用场景" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis应用场景<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis应用场景面试题汇总" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis应用场景面试题汇总<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li><li><!--[--><a aria-label="Redis设计" class="vp-link nav-link vp-sidebar-link vp-sidebar-page nav-link vp-sidebar-link vp-sidebar-page" href="/my_blog/backEnd/Redis/Redis%E8%AE%BE%E8%AE%A1.html"><span class="font-icon icon iconfont icon-write" style=""></span>Redis设计<!----></a><ul class="vp-sidebar-sub-headers"></ul><!--]--></li></ul></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><span class="font-icon icon iconfont icon-mysql" style=""></span><span class="vp-sidebar-title">MySQL</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><span class="font-icon icon iconfont icon-java" style=""></span><span class="vp-sidebar-title">Java</span><span class="vp-arrow end"></span></button><!----></section></li><li><section class="vp-sidebar-group"><button class="vp-sidebar-heading clickable" type="button"><span class="font-icon icon iconfont icon-OS" style=""></span><span class="vp-sidebar-title">OS</span><span class="vp-arrow end"></span></button><!----></section></li></ul><!--[--><!----><!--]--></aside><!--[--><main id="main-content" class="vp-page"><!--[--><!--[--><!----><!--]--><!----><nav class="vp-breadcrumb disable"></nav><div class="vp-page-title"><h1><!---->Redis高可用</h1><div class="page-info"><span class="page-author-info" aria-label="作者🖊" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><a class="page-author-item" href="/" target="_blank" rel="noopener noreferrer">LGYNB</a></span><span property="author" content="LGYNB"></span></span><!----><!----><!----><span class="page-reading-time-info" aria-label="阅读时间⌛" data-balloon-pos="down"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>大约 56 分钟</span><meta property="timeRequired" content="PT56M"></span><!----><!----></div><hr></div><div class="toc-place-holder"><aside id="toc"><!--[--><!----><!--]--><div class="toc-header">此页内容<button type="button" class="print-button" title="打印"><svg xmlns="http://www.w3.org/2000/svg" class="icon print-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="print icon"><path d="M819.2 364.8h-44.8V128c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v236.8h-44.8C145.067 364.8 96 413.867 96 473.6v192c0 59.733 49.067 108.8 108.8 108.8h44.8V896c0 17.067 14.933 32 32 32h460.8c17.067 0 32-14.933 32-32V774.4h44.8c59.733 0 108.8-49.067 108.8-108.8v-192c0-59.733-49.067-108.8-108.8-108.8zM313.6 160h396.8v204.8H313.6V160zm396.8 704H313.6V620.8h396.8V864zM864 665.6c0 25.6-19.2 44.8-44.8 44.8h-44.8V588.8c0-17.067-14.933-32-32-32H281.6c-17.067 0-32 14.933-32 32v121.6h-44.8c-25.6 0-44.8-19.2-44.8-44.8v-192c0-25.6 19.2-44.8 44.8-44.8h614.4c25.6 0 44.8 19.2 44.8 44.8v192z"></path></svg></button></div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#概念">概念</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#主从模式">主从模式</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#确定主从服务器">确定主从服务器</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#第一次同步">第一次同步</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#命令传播">命令传播</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#主服务器的压力分摊">主服务器的压力分摊</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#增量复制">增量复制</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#主从模式常见问题">主从模式常见问题</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#哨兵机制">哨兵机制</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#哨兵机制工作流程">哨兵机制工作流程</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#判断主节点是否故障">判断主节点是否故障</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#主从故障转移">主从故障转移</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#哨兵集群">哨兵集群</a></li><!----><!--]--></ul></li><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level2 toc-link level2" href="#分片集群">分片集群</a></li><li><ul class="toc-list"><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#分片集群特征">分片集群特征</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#分片集群的可扩展">分片集群的可扩展</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#集群的组合方式">集群的组合方式</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#集群数据的分片原理">集群数据的分片原理</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#client访问数据集群的过程">client访问数据集群的过程</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#数据复制过程和故障转移">数据复制过程和故障转移</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#分片集群的架构">分片集群的架构</a></li><!----><!--]--><!--[--><li class="toc-item"><a class="vp-link toc-link level3 toc-link level3" href="#rediscluster构建缓存集群的最佳实践">RedisCluster构建缓存集群的最佳实践</a></li><!----><!--]--></ul></li><!--]--></ul><div class="toc-marker" style="top:-1.7rem;"></div></div><!--[--><!----><!--]--></aside></div><!--[--><!----><!--]--><div class="theme-hope-content"><h1 id="redis高可用" tabindex="-1"><a class="header-anchor" href="#redis高可用" aria-hidden="true">#</a> Redis高可用</h1><h2 id="概念" tabindex="-1"><a class="header-anchor" href="#概念" aria-hidden="true">#</a> 概念</h2><blockquote><p>持久化是保存快照或日志记录在磁盘上，等待重启时恢复。但是如果机器故障了或者服务器宕机了，数据就会全没了</p></blockquote><p>为了避免单点故障通常的做法是将数据库复制多个副本以部署在不同的服务器上</p><p>这样即使有一台服务器出现故障，其他服务器依然可以继续提供服务。</p><p><strong>Redis 提供了复制（replication）功能，可以实现当一台数据库中的数据更新后，自动将更新的数据同步到其他数据库上</strong>。</p><p><strong>Redis三种集群模式</strong></p><ul><li>主从模式</li><li>哨兵模式(Sentinel)</li><li>分片集群(Cluster)</li></ul><h2 id="主从模式" tabindex="-1"><a class="header-anchor" href="#主从模式" aria-hidden="true">#</a> 主从模式</h2><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/1460000022808581.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>主从模式下，Redis会将主数据库Master复制给从数据库Slave，当主数据库中的数据更新，Redis会自动将更新的数据同步到从数据库</p><ul><li>主数据库可以进行读写操作</li><li>从数据库只进行读操作(对客户端来说)</li><li>一个主数据库可以拥有多个从数据库，而一个从数据库只能拥有一个主数据库。</li></ul><p>主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。</p><blockquote><p>意思是说，客户端发的写操作只有主服务器可以接收，主服务器执行完之后将这条命令同步给从服务器，让从服务器也去执行</p></blockquote><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/2b7231b6aabb9a9a2e2390ab3a280b2d-20230309232920063.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p><strong>主从复制机制的目的</strong></p><ul><li>一个是读写分离，分担 &quot;master&quot; 的读写压力</li><li>一个是方便做容灾恢复</li></ul><p><strong>主从复制三种模式</strong></p><ul><li>全量复制</li><li>基于长连接的命令传播</li><li>增量复制</li></ul><h3 id="确定主从服务器" tabindex="-1"><a class="header-anchor" href="#确定主从服务器" aria-hidden="true">#</a> 确定主从服务器</h3><p><code>replicaof</code>命令(Redis5.0之前是<code>slaveof</code>)来形成主从服务器的关系</p><p>服务器 A 和 服务器 B，在服务器 B 上执行下面这条命令：</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code><span class="token comment"># 服务器 B 执行这条命令</span>
replicaof <span class="token operator">&lt;</span>服务器 A 的 IP 地址<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>服务器 A 的 Redis 端口号<span class="token operator">&gt;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>服务器 B 就会变成服务器 A 的「从服务器」，然后与主服务器进行第一次同步。</p><h3 id="第一次同步" tabindex="-1"><a class="header-anchor" href="#第一次同步" aria-hidden="true">#</a> 第一次同步</h3><p>主从服务器间的第一次同步的过程可分为三个阶段：</p><ul><li>第一阶段是<strong>建立链接、协商同步</strong>；</li><li>第二阶段是<strong>主服务器同步数据给从服务器</strong>；</li><li>第三阶段是<strong>主服务器发送新写操作命令给从服务器</strong>。</li></ul><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/ea4f7e86baf2435af3999e5cd38b6a26.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h4 id="第一阶段-建立链接、协商同步" tabindex="-1"><a class="header-anchor" href="#第一阶段-建立链接、协商同步" aria-hidden="true">#</a> 第一阶段：建立链接、协商同步</h4><ol><li><p>从服务器执行<code>replicaof</code>命令，申请变成目标服务器的从服务器</p></li><li><p>执行<code>replicaof</code>命令之后，从服务器会向主服务器发送<code>psync</code>命令，<code>psync</code>命令包含两个参数</p><ul><li><p><code>runId</code>：每个Redis服务器在启动时都会自动生产一个随机的ID来唯一标识自己</p><ul><li>从服务器和主服务器第一次同步，从服务器不知道主服务器的<code>runId</code>，所以设置为<code>?</code></li></ul></li><li><p><code>offset</code>：表示复制的进度</p><ul><li>第一次同步时，值为<code>-1</code></li></ul></li></ul></li><li><p>主服务器收到<code>psync</code>命令之后，会用<code>FULLRESYNC</code>作为响应命令返回给对方，<code>FULLRESYNC</code>命令包含两个参数</p><ul><li>主服务器的<code>runId</code>和当前的复制进度<code>offset</code></li><li><code>FULLRESYNC</code>的意图是采用全量复制的方式，也就是主服务器将所有数据同步给从服务器</li></ul></li><li><p>从服务器接收到主服务器的<code>FULLRESYNC</code>命令之后，会保存主服务器的信息</p></li></ol><h4 id="第二阶段-主服务器同步数据给从服务器" tabindex="-1"><a class="header-anchor" href="#第二阶段-主服务器同步数据给从服务器" aria-hidden="true">#</a> 第二阶段：主服务器同步数据给从服务器</h4><ol><li>主服务器在发完<code>FULLRESYNC</code>命令之后，会执行<code>bgsave</code>命令来生成RDB文件，然后把文件发送给从服务器</li><li>从服务器接收到RDB文件之后，会先清空当前的数据，然后载入主服务器发送的RDB文件 <ul><li>主服务器生成 RDB 这个过程是不会阻塞主线程的，因为 bgsave 命令是产生了一个子进程来做生成 RDB 文件的工作，是异步工作的，这样 Redis 依然可以正常处理命令</li><li>但是，在这个期间主服务器接收到的写操作命令并没有记录到传过去的RDB文件中，所以此时主从服务器之间数据就不一致了</li></ul></li></ol><p>为了保证主从服务器的数据一致性，<strong>主服务器在下面这三个时间间隙中将收到的写操作命令，写入到 <code>replication buffer</code> 缓冲区里</strong>：</p><ul><li>主服务器生成 RDB 文件期间；</li><li>主服务器发送 RDB 文件给从服务器期间；</li><li>从服务器加载 RDB 文件期间；</li></ul><h4 id="第三阶段-主服务器发送新写操作命令给从服务器" tabindex="-1"><a class="header-anchor" href="#第三阶段-主服务器发送新写操作命令给从服务器" aria-hidden="true">#</a> 第三阶段：主服务器发送新写操作命令给从服务器</h4><ol><li>从服务器完成RDB载入后，会回复一个确认消息给主服务器</li><li>主服务器收到确认消息之后，会将<code>replication buffer</code>缓冲区里面所记录的写操作命令发送给从服务器，从服务器执行来自主服务器<code>replication buffer</code>缓冲区里发来的命令，这时主从服务器的数据就一致了</li></ol><p>至此，主从服务器的第一次同步就完成了</p><h3 id="命令传播" tabindex="-1"><a class="header-anchor" href="#命令传播" aria-hidden="true">#</a> 命令传播</h3><p>主从服务器在完成第一次同步后，双方之间会建立一个<strong>TCP连接</strong></p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/03eacec67cc58ff8d5819d0872ddd41e.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>后续主服务器可以通过这个连接继续将写操作命令传播给从服务器，然后从服务器执行该命令，使得与主服务器的数据库状态相同。</p><p>这个连接是长连接的，目的是避免频繁的 TCP 连接和断开带来的性能开销。</p><p>上面的这个过程被称为<strong>基于长连接的命令传播</strong>，通过这种方式来保证<strong>第一次同步后的主从服务器的数据一致性</strong>。</p><blockquote><p>基于长连接的命令传播中间不会经过全量复制和增量复制的repl缓冲区，而是会直接使用到TCP连接自己维护的缓冲区</p></blockquote><h3 id="主服务器的压力分摊" tabindex="-1"><a class="header-anchor" href="#主服务器的压力分摊" aria-hidden="true">#</a> 主服务器的压力分摊</h3><h4 id="主服务器全量同步的缺点" tabindex="-1"><a class="header-anchor" href="#主服务器全量同步的缺点" aria-hidden="true">#</a> 主服务器全量同步的缺点</h4><p>主从服务器在第一次数据同步的过程中，主服务器会做两件耗时的操作：<strong>生成 RDB 文件和传输 RDB 文件</strong>。</p><p>主服务器是可以有多个从服务器的，如果从服务器数量非常多，而且都与主服务器进行全量同步的话，就会带来两个问题：</p><ul><li>由于是通过 bgsave 命令来生成 RDB 文件的，那么主服务器就会忙于使用 fork() 创建子进程，如果主服务器的内存数据非大，在执行 fork() 函数时是会阻塞主线程的，从而使得 Redis 无法正常处理请求；</li><li>传输 RDB 文件会占用主服务器的网络带宽，会对主服务器响应命令请求产生影响。</li></ul><h4 id="解决问题" tabindex="-1"><a class="header-anchor" href="#解决问题" aria-hidden="true">#</a> 解决问题</h4><p><strong>从服务器获得自己的从服务器</strong></p><p>从服务器不仅可以接收主服务器的同步数据，自己也可以同时作为主服务器的形式将数据同步给从服务器</p><p>像组织架构一样一层一层往下分权，可以很好的减轻主服务器的压力</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/4d850bfe8d712d3d67ff13e59b919452.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h3 id="增量复制" tabindex="-1"><a class="header-anchor" href="#增量复制" aria-hidden="true">#</a> 增量复制</h3><p>主从服务器在完成第一次同步后，就会基于长连接进行命令传播。</p><p>但是基于网络进行的传输是不靠谱的</p><p>如果主从服务器间的网络连接断开了，那么就无法进行命令传播了，这时服务器的数据就没法和主服务器保持一致了，客户端就可能从从服务器读到旧的数据</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/4845008abadaa871613873f5ffdcb542.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><blockquote><p>如果断开的网络重新连接上了，应该如何继续保证主从服务器的数据一致性呢？</p></blockquote><p>Redis2.8之前，会重新进行一次全量复制，但很明显这样开销太大了</p><p>Redis2.8之后，网络断开又恢复后，主从服务器会采用增量复制的方式继续同步，也就是只会把网络断开期间主服务器接收的写操作命令同步给从服务器</p><h4 id="增量复制流程" tabindex="-1"><a class="header-anchor" href="#增量复制流程" aria-hidden="true">#</a> 增量复制流程</h4><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/e081b470870daeb763062bb873a4477e.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><ol><li>从服务器在恢复网络后，会发送<code>psync</code>命令给主服务器，此时<code>psync</code>命令中的<code>offset</code>参数不是-1</li><li>主服务器收到该命令后，用<code>CONTINUE</code>响应命令告诉从服务器接下来采用增量复制的方式同步数据</li><li>如何主服务将主从服务器断线期间所执行的写命令发送给从服务器，如何从服务器执行这些命令</li></ol><h4 id="增量数据" tabindex="-1"><a class="header-anchor" href="#增量数据" aria-hidden="true">#</a> 增量数据</h4><blockquote><p>主服务器如何存储断线期间执行的写命令呢？</p></blockquote><p>主服务器将增量数据存储在<code>repl_backlog_buffer</code>缓冲区中，并用<code>replication offset</code>标记缓冲区的同步进度</p><ul><li><strong>repl_backlog_buffer</strong>，是一个「<strong>环形</strong>」缓冲区，用于主从服务器断连后，从中找到差异的数据； <ul><li>主服务器在进行命令传播的时候，不仅会将写命令发送给从服务器，还会将命令写入到<code>repl_backlog_buffer</code>缓冲区中，因此这个缓冲区会保存着最近传播的写命令</li></ul></li><li><strong>replication offset</strong>，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 <code>master_repl_offset</code> 来记录自己「写」到的位置，从服务器使用 <code>slave_repl_offset</code> 来记录自己「读」到的位置。</li></ul><blockquote><p>如何确定<code>repl_backlog_buffer</code>中保存有从服务器需要的数据</p></blockquote><p>当从服务器重连上主服务器时，从服务器会通过<code>psync</code>命令将自己的复制偏移量<code>slave_repl_offset</code>发送给主服务器，主服务器根据自己的<code>master_repl_offset</code>和<code>slave_repl_offset</code>之间的差距来决定从服务器执行哪种同步操作</p><ul><li>如果判断出从服务器要读取的数据还在 <code>repl_backlog_buffer</code> 缓冲区里，那么主服务器将采用<strong>增量同步</strong>的方式；</li><li>相反，如果判断出从服务器要读取的数据已经不存在 <code>repl_backlog_buffer</code> 缓冲区里，那么主服务器将采用<strong>全量同步</strong>的方式。</li></ul><p>当主服务器在 <code>repl_backlog_buffer</code>中找到主从服务器差异(增量)的数据后，就会将增量的数据写入到 <code>replication buffer</code> 缓冲区</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/2db4831516b9a8b79f833cf0593c1f12.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><h4 id="环形缓冲区" tabindex="-1"><a class="header-anchor" href="#环形缓冲区" aria-hidden="true">#</a> 环形缓冲区</h4><p><code>repl_backlog_buffer</code> 缓冲区的默认大小是 1M，并且由于它是一个环形缓冲区，所以当缓冲区写满后，主服务器继续写入的话，就会覆盖之前的数据。因此，当主服务器的写入速度远超于从服务器的读取速度，缓冲区的数据一下就会被覆盖。</p><p>在网络恢复时，如果从服务器想读的数据已经被覆盖了，主服务器就会采用全量同步，这个方式比增量同步的性能损耗要大很多。</p><p><strong>为了避免在网络恢复时，主服务器频繁地使用全量同步的方式，应该调整下repl_backlog_buffer 缓冲区大小， 使其尽可能的大一些</strong>，减少出现从服务器要读取的数据被覆盖的概率，从而使得主服务器采用增量同步的方式。</p><p>repl_backlog_buffer 最小的大小可以根据公式估算</p><p>$$ second*write_size_per_second $$</p><ul><li><code>second</code>：从服务器断线后重新连接上主服务器所需的平均时间(以秒计算)。</li><li><code>write_size_per_second</code>：是主服务器平均每秒产生的写命令数据量大小。</li></ul><blockquote><p>如果主服务器平均每秒产生 1 MB 的写命令，而从服务器断线之后平均要 5 秒才能重新连接主服务器。</p><p>那么 <code>repl_backlog_buffer</code> 大小就不能低于 5 MB，否则新写地命令就会覆盖旧数据了。</p><p>当然，为了应对一些突发的情况，可以将 <code>repl_backlog_buffer</code> 的大小设置为此基础上的 2 倍，也就是 10 MB。</p></blockquote><p>修改<code>repl_backlog_buffer</code>的方法</p><div class="language-conf line-numbers-mode" data-ext="conf"><pre class="language-conf"><code>repl-backlog-size 1mb
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h4 id="主从复制优缺点" tabindex="-1"><a class="header-anchor" href="#主从复制优缺点" aria-hidden="true">#</a> 主从复制优缺点</h4><h5 id="主从复制优点" tabindex="-1"><a class="header-anchor" href="#主从复制优点" aria-hidden="true">#</a> 主从复制优点</h5><ul><li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离；</li><li>为了分载 Master 的读操作压力，Slave 服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成；</li><li>Slave 同样可以接受其它 Slaves 的连接和同步请求，这样可以有效的分载 Master 的同步压力；</li><li>Master Server 是以非阻塞的方式为 Slaves 提供服务。所以在 Master-Slave 同步期间，客户端仍然可以提交查询或修改请求；</li><li>Slave Server 同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据；</li></ul><h5 id="主从复制缺点" tabindex="-1"><a class="header-anchor" href="#主从复制缺点" aria-hidden="true">#</a> 主从复制缺点</h5><ul><li>Redis不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复（<strong>也就是要人工介入</strong>）；</li><li>主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性；</li><li>如果多个 Slave 断线了，需要重启的时候，尽量不要在同一时间段进行重启。因为只要 Slave 启动，就会发送sync 请求和主机全量同步，当多个 Slave 重启的时候，可能会导致 Master IO 剧增从而宕机。</li><li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂；</li></ul><h3 id="主从模式常见问题" tabindex="-1"><a class="header-anchor" href="#主从模式常见问题" aria-hidden="true">#</a> 主从模式常见问题</h3><h4 id="如何判断节点是否正常工作" tabindex="-1"><a class="header-anchor" href="#如何判断节点是否正常工作" aria-hidden="true">#</a> 如何判断节点是否正常工作</h4><p>Redis 判断节点是否正常工作，基本都是通过互相的 ping-pong 心态检测机制，<strong>如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接</strong>。</p><p>Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别：</p><ul><li>Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数<code>repl-ping-slave-period</code>控制发送频率。</li><li>Redis 从节点每隔 1 秒发送 <code>replconf ack{offset}</code> 命令，给主节点上报自身当前的复制偏移量，目的是为了： <ul><li>实时监测主从节点网络状态；</li><li>上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据。</li></ul></li></ul><h4 id="主从模式中的过期key" tabindex="-1"><a class="header-anchor" href="#主从模式中的过期key" aria-hidden="true">#</a> 主从模式中的过期Key</h4><p>在Redis主从模式下，从库不会进行过期扫描，从库对过期的处理是被动的</p><ul><li>也就是即使从库中的key过期了，如果有客户端访问从库时，依然可以得到key的值，像未过期的键值对一样返回</li></ul><p>主节点处理了一个过期key，在这个时间主节点会模拟一条del命令发送给从节点</p><p>从节点收到该命令后，就进行删除key的操作。</p><h4 id="主从复制的两个buf的区别" tabindex="-1"><a class="header-anchor" href="#主从复制的两个buf的区别" aria-hidden="true">#</a> 主从复制的两个buf的区别</h4><ul><li><p>出现的阶段不一样</p><ul><li><code>repl_backlog_buffer</code>是在增量复制阶段出现，一个主节点只分配一个<code>repl_backlog_buffer</code></li><li><code>repl_buffer</code>在全量复制，增量复制中出现，主节点会给每个新连接的从节点分配一个<code>repl_buffer</code></li></ul></li><li><p>两个buf都有大小限制，缓冲区满了之后</p><ul><li><code>repl_backlog_buffer</code>是环形结构的，新数据会直接覆盖掉初始位置的数据</li><li><code>repl_buffer</code>会导致连接断开，删除缓存，从节点重新连接，重新开始全量复制</li></ul></li><li><p>主节点每次收到写命令之后，会同时写到<code>repl_buffer</code>和<code>repl_backlog_buffer</code>，然后异步将<code>repl_buffer</code>中的命令发给从服务器</p></li></ul><h4 id="如何应对主从数据不一致" tabindex="-1"><a class="header-anchor" href="#如何应对主从数据不一致" aria-hidden="true">#</a> 如何应对主从数据不一致</h4><blockquote><p>为什么会出现主从数据不一致</p></blockquote><p>主从数据不一致，就是指客户端从从节点中读取到的值和主节点中的最新值并不一致。</p><p><strong>因为主从节点间的命令复制是异步进行的</strong>，无法实现强一致性保证（主从数据时刻保持一致）。</p><ul><li>具体来说，在主从节点命令传播阶段，主节点收到新的写命令后，会发送给从节点。</li><li>主节点并不会等到从节点实际执行完命令后，再把结果返回给客户端。主节点自己在本地执行完命令后，就会向客户端返回结果了</li><li>如果从节点还没有执行主节点同步过来的命令，主从节点间的数据就不一致了。</li></ul><blockquote><p>如何应对主从数据不一致</p></blockquote><p>第一种方法，尽量保证主从节点间的网络连接状况良好，避免主从节点在不同的机房。</p><p>第二种方法，可以开发一个外部程序来监控主从节点间的复制进度。具体做法：</p><ul><li>Redis 的 INFO replication 命令可以查看主节点接收写命令的进度信息（master_repl_offset）和从节点复制写命令的进度信息（slave_repl_offset），所以，我们就可以开发一个监控程序，先用 INFO replication 命令查到主、从节点的进度，然后，我们用 master_repl_offset 减去 slave_repl_offset，这样就能得到<strong>从节点和主节点间的复制进度差值</strong>了。</li><li>这时候可以设定一个阈值，来控制业务最大可以接受的数据不一致程度，如果某个从节点的进度差值大于我们预设的阈值，我们可以让客户端不再和这个从节点连接进行数据读取，这样就可以减少读到不一致数据的情况</li><li>不过，为了避免出现客户端和所有从节点都不能连接的情况，我们需要把复制进度差值的阈值设置得大一些。</li></ul><h4 id="主从切换数据丢失" tabindex="-1"><a class="header-anchor" href="#主从切换数据丢失" aria-hidden="true">#</a> 主从切换数据丢失</h4><p>产生数据丢失的情况</p><ul><li>异步复制同步丢失</li><li>集群产生脑裂数据丢失</li></ul><blockquote><p>无法真正做到数据完全不丢失，只能尽可能少丢失</p></blockquote><h5 id="异步复制同步丢失" tabindex="-1"><a class="header-anchor" href="#异步复制同步丢失" aria-hidden="true">#</a> 异步复制同步丢失</h5><p>由于主从节点的数据同步是异步的，并且是在主节点执行完命令之后才异步发送出去的</p><p>如果主节点执行完命令之后挂了，同步命令没有发送出去，那此时主节点中新增的数据就会丢失</p><blockquote><p><strong>减少异步复制数据丢失的方案</strong></p></blockquote><p><code>min-slaves-max-lag</code>参数可以配置丢失的数据差最小时间</p><ul><li><p>这个参数表示：一旦所有的从节点数据复制和同步的延迟都超过了设置的值，主节点就会拒接接收任何请求</p><ul><li>假设将 min-slaves-max-lag 配置为 10s 后，master 就拒绝写入新请求</li><li>这样就能将 master 和 slave 数据差控制在10s内，即使 master 宕机也只是这未复制的 10s 数据。</li><li>对于客户端，当客户端发现 master 不可写后，我们可以采取降级措施，将数据暂时写入本地缓存和磁盘中，在一段时间（等 master 恢复正常）后重新写入 master 来保证数据不丢失，也可以将数据写入 kafka 消息队列，等 master 恢复正常，再隔一段时间去消费 kafka 中的数据，让将数据重新写入 master 。</li></ul></li><li><p>主节点执行完写请求之后，发送给从节点的同步命令由于网络等原因发送延迟或阻塞</p><ul><li>此时如果没有这个参数，那卡多久就丢失多久的数据，配置了参数之后，只会丢失配置的时间内的数据。</li></ul></li><li><p>这个参数设置的时间相当于一个大保底，无论什么情况，主从节点的数据不一致不会小于配置的时间之内</p></li></ul><h5 id="集群产生脑裂数据丢失" tabindex="-1"><a class="header-anchor" href="#集群产生脑裂数据丢失" aria-hidden="true">#</a> 集群产生脑裂数据丢失</h5><blockquote><p><strong>集群脑裂现象</strong></p></blockquote><p>在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。</p><p>如果主节点的网络突然发生了问题，它与所有的从节点都失联了，但是此时的主节点和客户端的网络是正常的，这个客户端并不知道 Redis 内部已经出现了问题，还在照样的向这个失联的主节点写数据（过程A），此时这些数据被主节点缓存到了缓冲区里，因为主从节点之间的网络问题，这些数据都是无法同步给从节点的。</p><p>这时，哨兵也发现主节点失联了，它就认为主节点挂了（但实际上主节点正常运行，只是网络出问题了），于是哨兵就会在从节点中选举出一个 leeder 作为主节点，这时集群就有两个主节点了 —— <strong>脑裂出现了</strong>。</p><p>这时候网络突然好了，哨兵因为之前已经选举出一个新主节点了，它就会把旧主节点降级为从节点（A），然后从节点（A）会向新主节点请求数据同步，<strong>因为第一次同步是全量同步的方式，此时的从节点（A）会清空掉自己本地的数据，然后再做全量同步。所以，之前客户端在过程 A 写入的数据就会丢失了，也就是集群产生脑裂数据丢失的问题</strong>。</p><p>意思就是说：由于网络问题，集群节点之间失去联系。主从数据不同步，重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了</p><blockquote><p><strong>减少脑裂数据丢失的方案</strong></p></blockquote><p>当主节点发现「从节点下线的数量太多」，或者「网络延迟太大」的时候，那么主节点会禁止写操作，直接把错误返回给客户端。</p><p>在 Redis 的配置文件中有两个参数我们可以设置：</p><ul><li><code>min-slaves-to-write N</code>，主节点必须要有<strong>至少 N 个从节点连接</strong>，如果小于这个数，主节点会禁止写数据。</li><li><code>min-slaves-max-lag T</code>，主从数据复制和同步的延迟<strong>不能超过 T 秒</strong>，如果主从同步的延迟超过 T 秒，主节点会禁止写数据。</li></ul><p>这两个配置项组合后的要求是，<strong>主节点连接的从节点中至少有 N 个从节点，「并且」主节点进行数据复制时的 ACK 消息延迟不能超过 T 秒</strong>，否则，主节点就不会再接收客户端的写请求了。<strong>等到新主节点上线时，就只有新主节点能接收和处理客户端请求，此时，新写的数据会被直接写到新主节点中。而原主节点会被哨兵降为从节点，即使它的数据被清空了，也不会有新数据丢失。</strong></p><h4 id="主从如何做到故障自动切换" tabindex="-1"><a class="header-anchor" href="#主从如何做到故障自动切换" aria-hidden="true">#</a> 主从如何做到故障自动切换</h4><p>主节点挂了 ，从节点是无法自动升级为主节点的，这个过程需要人工处理，在此期间 Redis 无法对外提供写操作。</p><p>此时，Redis 哨兵机制就登场了，哨兵在发现主节点出现故障时，由<strong>哨兵自动完成故障发现和故障转移，并通知给应用方</strong>，从而实现高可用性</p><h2 id="哨兵机制" tabindex="-1"><a class="header-anchor" href="#哨兵机制" aria-hidden="true">#</a> 哨兵机制</h2><p><strong>哨兵机制（Sentinel）</strong>，它的作用是实现<strong>主从节点故障转移</strong>。它会监测主节点是否存活，如果发现主节点挂了，它就会选举一个从节点切换为主节点，并且把新主节点的相关信息通知给从节点和客户端。</p><h3 id="哨兵机制工作流程" tabindex="-1"><a class="header-anchor" href="#哨兵机制工作流程" aria-hidden="true">#</a> 哨兵机制工作流程</h3><p>哨兵其实是一个运行在特殊模式下的Redis进程，它也是一个节点</p><p>哨兵相当于观察者节点，观察的对象是主从节点</p><p>在它观察到有异常状态之后，会做出一些动作，来修复异常</p><p>哨兵节点主要负责三件事情</p><ul><li>监控</li><li>选主</li><li>通知</li></ul><p>为了减少误判的情况，哨兵在部署的时候不会只部署一个节点，而是用多个节点部署成<strong>哨兵集群</strong>（<em>最少需要三台机器来部署哨兵集群</em>）</p><p><strong>通过多个哨兵节点一起判断，就可以就可以避免单个哨兵因为自身网络状况不好，而误判主节点下线的情况</strong>。</p><p>同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。</p><h3 id="判断主节点是否故障" tabindex="-1"><a class="header-anchor" href="#判断主节点是否故障" aria-hidden="true">#</a> 判断主节点是否故障</h3><h4 id="主观下线" tabindex="-1"><a class="header-anchor" href="#主观下线" aria-hidden="true">#</a> 主观下线</h4><p>哨兵每个1秒会给所有的主从节点发送PING命令</p><p>主从节点收到PING命令之后会发送一个响应命令给哨兵，从而让哨兵知道它们在正常运行</p><p>如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「<strong>主观下线</strong>」。</p><p>这个「规定的时间」是配置项 <code>down-after-milliseconds</code> 参数设定的，单位是毫秒。</p><h4 id="客观下线" tabindex="-1"><a class="header-anchor" href="#客观下线" aria-hidden="true">#</a> 客观下线</h4><p>客观下线只适用于主节点</p><blockquote><p>针对「主节点」设计「主观下线」和「客观下线」两个状态，是因为有可能「主节点」其实并没有故障，可能只是因为主节点的系统压力比较大或者网络发送了拥塞，导致主节点没有在规定时间内响应哨兵的 PING 命令。</p></blockquote><p>当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应。</p><p>当这个哨兵的赞同票数达到哨兵配置文件中的 <code>quorum</code> 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/13e4361407ba46979e802eaa654dcf67.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>例如，现在有 3 个哨兵，<code>quorum</code> 配置的是 2，那么一个哨兵需要 2 张赞成票，就可以标记主节点为“客观下线”了。这 2 张赞成票包括哨兵自己的一张赞成票和另外两个哨兵的赞成票。</p><p><code>quorum</code> 的值一般设置为哨兵个数的二分之一加 1，例如 3 个哨兵就设置 2。</p><p>哨兵判断完主节点客观下线后，哨兵就要开始在多个「从节点」中，选出一个从节点来做新主节点</p><h3 id="主从故障转移" tabindex="-1"><a class="header-anchor" href="#主从故障转移" aria-hidden="true">#</a> 主从故障转移</h3><p>由哨兵的leader来进行主从故障转移</p><h4 id="如何选取哨兵leader" tabindex="-1"><a class="header-anchor" href="#如何选取哨兵leader" aria-hidden="true">#</a> 如何选取哨兵leader</h4><blockquote><p><strong>谁是候选者</strong></p></blockquote><p>哪个哨兵节点判断主节点为「客观下线」，这个哨兵节点就是候选者，所谓的候选者就是想当 Leader 的哨兵。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/d0bed80d28a543fd8dcd299d4b06cf04.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>假设有三个哨兵。当哨兵 B 先判断到主节点「主观下线后」，就会给其他实例发送 <code>is-master-down-by-addr</code> 命令。接着，其他哨兵会根据自己和主节点的网络连接情况，做出赞成投票或者拒绝投票的响应。</p><p>当哨兵 B 收到赞成票数达到哨兵配置文件中的 <code>quorum</code> 配置项设定的值后，就会将主节点标记为「客观下线」，此时的哨兵 B 就是一个 Leader 候选者。</p><p>候选者会向其他哨兵发送命令，表明希望成为 Leader 来执行主从切换，并让所有其他哨兵对它进行投票。</p><p>每个哨兵只有一次投票机会，如果用完后就不能参与投票了，可以投给自己或投给别人，但是只有候选者才能把票投给自己。</p><blockquote><p><strong>成功当选leader的条件</strong></p></blockquote><p>那么在投票过程中，任何一个「候选者」，要满足两个条件：</p><ul><li>第一，拿到<strong>半数以上的赞成票</strong>；</li><li>第二，拿到的票数同时还需要<strong>大于等于哨兵配置文件中的 <code>quorum</code> 值</strong>。</li></ul><p>举个例子，假设哨兵节点有 3 个，<code>quorum</code> 设置为 2，那么任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以选举成功了。如果没有满足条件，就需要重新进行选举。</p><blockquote><p><strong>leader大选投票流程</strong></p></blockquote><ol><li>每位候选者给自己投一票</li><li>发送投票请求给其他的哨兵节点</li><li>非候选者的哨兵节点先收到哪个候选者的投票请求就给谁投票</li><li>用完投票机会之后再收到投票请求就会拒绝投票</li></ol><blockquote><p><strong>为什么哨兵节点至少要有3个</strong></p></blockquote><p>如果哨兵集群中只有 2 个哨兵节点，此时如果一个哨兵想要成功成为 Leader，必须获得 2 票，而不是 1 票。</p><p>所以，如果哨兵集群中有个哨兵挂掉了，那么就只剩一个哨兵了，如果这个哨兵想要成为 Leader，这时票数就没办法达到 2 票，就<strong>无法成功成为 Leader，这时是无法进行主从节点切换的</strong>。</p><p>因此，通常我们至少会配置 3 个哨兵节点。这时，如果哨兵集群中有个哨兵挂掉了，那么还剩下两个哨兵，如果这个哨兵想要成为 Leader，这时还是有机会达到 2 票的，所以还是可以选举成功的，不会导致无法进行主从节点切换。</p><p>如果 3 个哨兵节点，挂了 2 个怎么办？这个时候得<strong>人为介入了，或者增加多一点哨兵节点</strong>。</p><blockquote><p><strong>为什么quorum的值建议是哨兵个数的二分之一加1</strong></p></blockquote><p>Redis1主4从，5个哨兵，quorum 设置为3，如果2个哨兵故障，当主节点宕机时，哨兵能否判断主节点客观下线？主从能否自动切换？</p><ul><li><strong>哨兵集群可以判定主节点“客观下线”</strong>。哨兵集群还剩下 3 个哨兵，当一个哨兵判断主节点“主观下线”后，询问另外 2 个哨兵后，有可能能拿到 3 张赞同票，这时就达到了 quorum 的值，因此，哨兵集群可以判定主节点为“客观下线”。</li><li><strong>哨兵集群可以完成主从切换</strong>。当有个哨兵标记主节点为「客观下线」后，就会进行选举 Leader 的过程，因为此时哨兵集群还剩下 3 个哨兵，那么还是可以拿到半数以上（5/2+1=3）的票，而且也达到了 quorum 值，满足了选举 Leader 的两个条件，所以就能选举成功，因此哨兵集群可以完成主从切换。</li></ul><p>如果 quorum 设置为 2，并且如果有 3 个哨兵故障的话。此时可以判定主节点为“客观下线”，但是不能完成主从切换</p><p>如果 quorum 设置为 3，并且如果有 3 个哨兵故障的话。不能判定主节点为“客观下线”，也不能完成主从切换</p><blockquote><p>可以看到，quorum 为 2 的时候，并且如果有 3 个哨兵故障的话，虽然可以判定主节点为“客观下线”，但是不能完成主从切换，这样感觉「判定主节点为客观下线」这件事情白做了一样，既然这样，还不如不要做，quorum 为 3 的时候，就可以避免这种无用功。</p></blockquote><p><strong>quorum 的值建议设置为哨兵个数的二分之一加 1，而且哨兵节点的数量应该是奇数</strong>。</p><ul><li>例如 3 个哨兵就设置 2，5 个哨兵设置为 3</li></ul><h4 id="主从故障转移的流程" tabindex="-1"><a class="header-anchor" href="#主从故障转移的流程" aria-hidden="true">#</a> 主从故障转移的流程</h4><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/主从故障转移.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>主从故障转移操作主要包含四个步骤</p><ol><li>在已下线的主节点（旧主节点）属下的所有从节点里面挑选一个从节点，并将其转换为主节点</li><li>旧主节点属下的所有从节点修改主从复制目标，修改为复制新主节点</li><li>将新主节点的IP地址和信息通过发布者/订阅者机制通知给客户端</li><li>继续监视旧主节点，旧主节点重新上线时，将其设置为新主节点的从节点</li></ol><h5 id="选出新主节点" tabindex="-1"><a class="header-anchor" href="#选出新主节点" aria-hidden="true">#</a> 选出新主节点</h5><blockquote><p><strong>新主节点选举流程</strong></p></blockquote><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/选主过程.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>首先要把<strong>网络状态不好的从节点给过滤掉</strong>。</p><ul><li>过滤掉已经下线的从节点</li><li>过滤掉以往网络连接状态不好的从节点</li></ul><p>Redis 有个叫 <code>down-after-milliseconds * 10</code> 配置项（主从节点断连的最大连接超时时间），</p><ul><li>如果在 <code>down-after-milliseconds</code> 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了</li><li>如果发生断连的次数超过了 <code>10</code> 次，就说明这个从节点的网络状况不好，不适合作为新主节点。</li></ul><p>接下来要对所有从节点进行三轮考察：<strong>优先级、复制进度、ID 号</strong>。</p><p>在进行每一轮考察的时候，哪个从节点优先胜出，就选择其作为新主节点。</p><ul><li>第一轮考察：哨兵首先会根据从节点的优先级来进行排序，优先级越小排名越靠前，</li><li>第二轮考察：如果优先级相同，则查看复制的下标，哪个从「主节点」接收的复制数据多，哪个就靠前。</li><li>第三轮考察：如果优先级和下标都相同，就选择从节点 ID 较小的那个。</li></ul><blockquote><p>第一轮考察不出来才会进入第二轮考察，第二轮考察不出来才会进入第三轮考察</p></blockquote><p><strong>第一轮考察：优先级最高的从节点胜出</strong></p><p>Redis 有个叫 <code>slave-priority</code> 配置项，可以给从节点设置优先级。</p><p>每一台从节点的服务器配置不一定是相同的，我们可以根据服务器性能配置来设置从节点的优先级。</p><p>比如，如果「A 从节点」的物理内存是所有从节点中最大的，那么我们可以把「A 从节点」的优先级设置成最高。这样当哨兵进行第一轮考虑的时候，优先级最高的 A 从节点就会优先胜出，于是就会成为新主节点。</p><p><strong>第二轮考察：复制进度最靠前的从节点胜出</strong></p><p>如果在第一轮考察中，发现优先级最高的从节点有两个，那么就会进行第二轮考察，比较两个从节点哪个复制进度。</p><p><code>master_repl_offset</code>减去<code>slave_repl_offset</code> 就是复制进度</p><p>如果某个从节点的 <code>slave_repl_offset</code> 最接近 <code>master_repl_offset</code>，说明它的复制进度是最靠前的，于是就可以将它选为新主节点。</p><p><strong>第三轮考察：ID 号小的从节点胜出</strong></p><p>如果在第二轮考察中，发现有两个从节点优先级和复制进度都是一样的，那么就会进行第三轮考察，比较两个从节点的 ID 号，ID 号小的从节点胜出。</p><p>每个从节点都有一个编号，这个编号就是 ID 号，是用来唯一标识从节点的。</p><blockquote><p><strong>升级流程</strong></p></blockquote><p>在选举出从节点后，哨兵 leader向被选中的从节点发送 <code>SLAVEOF no one</code> 命令，让从节点<strong>解除从节点的身份，将其升级为新主节点</strong>。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/升级从节点为主节点.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>在发送 <code>SLAVEOF no one</code> 命令之后，哨兵 leader 会以每秒一次的频率向被升级的从节点发送 <code>INFO</code> 命令（没进行故障转移之前，<code>INFO</code> 命令的频率是每十秒一次），并观察命令回复中的角色信息，当被升级节点的角色信息从原来的 slave 变为 master 时，哨兵 leader 就知道被选中的从节点已经顺利升级为主节点了。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/升主节点成功.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h5 id="将从节点指向新节点" tabindex="-1"><a class="header-anchor" href="#将从节点指向新节点" aria-hidden="true">#</a> 将从节点指向新节点</h5><p>当新主节点出现之后，哨兵 leader 下一步要做的就是，让已下线主节点属下的所有「从节点」指向「新主节点」，这一动作可以通过向「从节点」发送 <code>SLAVEOF</code> 命令来实现。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/从节点指向新主节点.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/从节点转换成功.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h5 id="通知客户端主节点更换" tabindex="-1"><a class="header-anchor" href="#通知客户端主节点更换" aria-hidden="true">#</a> 通知客户端主节点更换</h5><p>哨兵集群终于完成主从切换的工作，那么新主节点的信息要如何通知给客户端呢？</p><p>这主要<strong>通过 Redis 的发布者/订阅者机制来实现</strong>的。每个哨兵节点提供发布者/订阅者机制，客户端可以从哨兵订阅消息。</p><p>哨兵提供的消息订阅频道有很多，不同频道包含了主从节点切换过程中的不同关键事件，几个常见的事件如下：</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/哨兵频道.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。<strong>主从切换完成后，哨兵就会向 <code>+switch-master</code> 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了</strong>。</p><p>通过发布者/订阅者机制机制，有了这些事件通知，客户端不仅可以在主从切换后得到新主节点的连接信息，还可以监控到主从节点切换过程中发生的各个重要事件。这样，客户端就可以知道主从切换进行到哪一步了，有助于了解切换进度。</p><h5 id="将旧主节点变为从节点" tabindex="-1"><a class="header-anchor" href="#将旧主节点变为从节点" aria-hidden="true">#</a> 将旧主节点变为从节点</h5><p>监视旧主节点，当旧主节点重新上线时，哨兵集群就会向它发送 <code>SLAVEOF</code> 命令，让它成为新主节点的从节点</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/旧主节点变为新主节点.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="哨兵集群" tabindex="-1"><a class="header-anchor" href="#哨兵集群" aria-hidden="true">#</a> 哨兵集群</h3><p>配置哨兵信息</p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>sentinel monitor <span class="token operator">&lt;</span>master-name<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>ip<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>redis-port<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>quorum<span class="token operator">&gt;</span> 
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><ul><li>只需要填写主节点名称，主节点IP地址，Redis端口号，quorum值</li><li>不需要填其他哨兵信息</li></ul><h4 id="哨兵之间相互发现" tabindex="-1"><a class="header-anchor" href="#哨兵之间相互发现" aria-hidden="true">#</a> 哨兵之间相互发现</h4><p>哨兵是通过<strong>发布者/订阅者</strong>机制来相互发现的</p><p>在主从集群中，主节点上有一个名为<code>__sentinel__:hello</code>的频道，不同哨兵就是通过这个频道来相互发现，实现相互通信的</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/a6286053c6884cf58bf397d01674fe80.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h4 id="哨兵集群监控从节点" tabindex="-1"><a class="header-anchor" href="#哨兵集群监控从节点" aria-hidden="true">#</a> 哨兵集群监控从节点</h4><p>主节点知道所有「从节点」的信息，所以哨兵会每 10 秒一次的频率向主节点发送 INFO 命令来获取所有「从节点」的信息。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/fdd5f695bb3643258662886f9fba0aab.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>正是通过 Redis 的发布者/订阅者机制，哨兵之间可以相互感知，然后组成集群，同时，哨兵又通过 INFO 命令，在主节点里获得了所有从节点连接信息，于是就能和从节点建立连接，并进行监控了。</p><h2 id="分片集群" tabindex="-1"><a class="header-anchor" href="#分片集群" aria-hidden="true">#</a> 分片集群</h2><p>将总数据划分为 16384（2的14次方）个哈希槽（slots）</p><p>如果你有多个实例节点，那么每个实例节点将管理其中一部分的槽位，槽位的信息会存储在各自所归属的节点中。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/D8FF624CDF1327C46B5687CF4ACD9956.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><ul><li><strong>一个Redis集群一共有16384个哈希槽</strong>，可以有1 ~ n个节点来分配这些哈希槽，<strong>可以不均匀分配，每个节点可以处理0个到至多 16384 个槽点</strong></li><li>当16384个哈希槽<strong>都有节点进行管理</strong>的时候，集群处于<strong>online</strong> 状态。同样的，如果<strong>有一个哈希槽没有被管理到</strong>，那么集群处于<strong>offline</strong>状态</li><li>4个实例节点组成了一个集群，集群之间的<strong>信息通过 <a href="https://www.jianshu.com/p/37231c0455a9?u_atoken=8f004d09-69b0-426f-b061-eb2beec366bd&amp;u_asession=01kPy2LmGTRcCRkywzTBYEHCkosMcq1sc9kA_QMxQzS0laSORdnXM_dgnvDG1wTbw8X0KNBwm7Lovlpxjd_P_q4JsKWYrT3W_NKPr8w6oU7K-N03wKh1fGpdgnBALMXOj6nHmbkqVcEgdObpAroqY1_GBkFo3NEHBv0PZUm6pbxQU&amp;u_asig=05mlGEb2mTIXVShMltoDiqa8mmSULaVgG2oC6fDPX8mGfiV1LZT1wLbYW94ZRRUgNN2m30YJvBI8QN0vcGpawzqQ56Wvq9ANJ8evvyrzij6204SAUiczKrC3sz7bqPpXbYaozWZDEauebiZYp_CIfFOT2ReLQHIkgXR5cKZYDZCf_9JS7q8ZD7Xtz2Ly-b0kmuyAKRFSVJkkdwVUnyHAIJzRlYRaI5LXI74XoQdbRrBZ3vA9ftOKk0zloJSVmQmIfB6xbSxAaWh9ph0bRUFW-6vO3h9VXwMyh6PgyDIVSG1W_7c_dy_6Dl1wgLC3Whv7tuy_Ck9NIynL3sfTheEXzY-rYacisV-aUEZiNNy5yxHlkwafvmAr9jpurPBo8emYF2mWspDxyAEEo4kbsryBKb9Q&amp;u_aref=tGICtqx3upDt17e1WtJxSOZFJWU%3D" target="_blank" rel="noopener noreferrer">Gossip协议<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a> 进行交互</strong>，这样就可以<strong>在某一节点记录其他节点的哈希槽（slots）的分配情况</strong>。</li></ul><h3 id="分片集群特征" tabindex="-1"><a class="header-anchor" href="#分片集群特征" aria-hidden="true">#</a> 分片集群特征</h3><ul><li>集群中有多个master，每个master保存不同数据</li><li>每个master都可以有多个slave节点</li><li>master之间通过ping监测彼此健康状态 <ul><li>这样就可以不用哨兵了，多个master之间就互相监测</li><li>如果多个master认为一个master主观下线从而该master客观下线，将来也可以做一个主从的切换</li></ul></li><li>客户端请求可以访问集群任意节点，最终都会被转发到正确节点，多个redis节点可以对请求进行路由</li></ul><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/image-20231119114412823.png" alt="image-20231119114412823" tabindex="0" loading="lazy"><figcaption>image-20231119114412823</figcaption></figure><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/1460000022808584.png" alt="image-20200531184321294" tabindex="0" loading="lazy"><figcaption>image-20200531184321294</figcaption></figure><h3 id="分片集群的可扩展" tabindex="-1"><a class="header-anchor" href="#分片集群的可扩展" aria-hidden="true">#</a> 分片集群的可扩展</h3><p>单机的吞吐无法承受持续扩增的流量的时候，好的办法是从横向（scale out） 和 纵向（scale up）两方面进行扩展</p><ul><li><strong>纵向</strong>扩展（scale up）：将单个实例的<strong>硬件资源</strong>做提升，比如 CPU核数量、内存容量、SSD容量。</li><li><strong>横向</strong>扩展（scale out）：横向<strong>扩增 Redis 实例数</strong>，这样<strong>每个节点只负责一部分数据就可以，分担一下压力</strong>，典型的分治思维。 <ul><li>每次往集群增加节点的时候，需要从集群的那些老节点中，搬运一些槽到新节点</li></ul></li></ul><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/7a512fec7eba789c6d098b834929701a-20221015223055-sy7dbyt.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><blockquote><p><strong>那横向扩展和纵向扩展各有什么优缺点呢？</strong></p></blockquote><ul><li><code>scale up</code> 虽然操作起来比较简易。但是没法解决Redis一些瓶颈问题，比如持久化（如轮式RDB快照还是AOF指令），遇到大数据量的时候，照样效率会很低，响应慢。另外，单台服务机硬件扩容也是有限制的，不可能无限操作。</li><li><code>scale out</code> 更容易扩展，分片的模式可以解决很多问题，包括单一实例节点的硬件扩容限制、成本限制，还可以分摊压力，精细化治理，精细化维护。但是同时也要面临分布式带来的一些问题</li><li>现实情况下，在面对千万级甚至亿级别的流量的时候，很多大厂都是在千百台的实例节点组成的集群上进行流量调度、服务治理的。所以，使用Cluster模式，是业内广泛采用的模式。</li></ul><h3 id="集群的组合方式" tabindex="-1"><a class="header-anchor" href="#集群的组合方式" aria-hidden="true">#</a> 集群的组合方式</h3><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/F5D8E5E24554A0CB84A33F54D92C0E31.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>集群是由一个个互相独立的节点（readis node）组成的， 所以刚开始的时候，他们都是隔离，毫无联系的。我们需要通过一些操作，把他们聚集在一起，终才能组成真正的可协调工作的集群</p><p>各个节点的联通是通过 CLUSTER MEET 命令完成的：<code>CLUSTER MEET &lt;ip&gt; &lt;port&gt;</code></p><p>具体的做法是其中一个node向另外一个 node（指定 ip 和 port） 发送 CLUSTER MEET 命令，这样就可以让两个节点进行握手(handshake) ，握手成功之后，node 节点就会将握手另一侧的节点添加到当前节点所在的集群中。</p><h3 id="集群数据的分片原理" tabindex="-1"><a class="header-anchor" href="#集群数据的分片原理" aria-hidden="true">#</a> 集群数据的分片原理</h3><h4 id="哈希槽-slots-的映射" tabindex="-1"><a class="header-anchor" href="#哈希槽-slots-的映射" aria-hidden="true">#</a> 哈希槽(slots)的映射</h4><p>整个Redis数据库划分为16384个哈希槽，Redis集群可能有n个实例节点</p><p>每个节点可以处理0个到至多 16384 个槽点，这些节点会把 16384个槽位瓜分完</p><p>而<strong>实际存储的Redis键值信息也必然归属于这 16384 个槽的其中一个</strong></p><p>slots 与 Redis Key 的映射是通过以下两个步骤完成的：</p><ul><li><p>使用 <strong>CRC16 算法计算键值对信息的Key，会得出一个 16 bit 的值</strong>。</p></li><li><p>将第1步中得到的 <strong>16 bit 的值对 16384 取模，得到的值会在 0 ～ 16383 之间，映射到对应到哈希槽中</strong>。</p></li><li><p>当然，可能在一些特殊的情况下，你想把某些key固定到某个slot上面，也就是同一个实例节点上。这时候<strong>可以用hash tag能力，强制 key 所归属的槽位等于 tag 所在的槽位</strong>。</p><blockquote><p>其实现方式为在key中加个{}，例如test_key{1}。使用hash tag后客户端在计算key的crc16时，只计算{}中数据。如果没使用hash tag，客户端会对整个key进行crc16计算</p></blockquote></li></ul><h4 id="哈希槽-slots-的划分" tabindex="-1"><a class="header-anchor" href="#哈希槽-slots-的划分" aria-hidden="true">#</a> 哈希槽(slots)的划分</h4><ul><li><strong>初始化时均匀分配slots</strong>：使用 <code>cluster create</code> 创建，会将 16384 个slots 平均分配在我们的集群实例上，比如你有n个节点，那每个节点的槽位就是 16384 / n 个了 。</li><li><strong>自定义节点的slots分配</strong><ul><li>使用<code>cluster meet</code>将节点们连在一起，形成集群</li><li>使用<code>cluster addslots</code>来为它们分配slots</li><li>可以为性能好的多份配点slots</li></ul></li></ul><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>redis-cli <span class="token parameter variable">-h</span> <span class="token number">192.168</span>.0.1 –p <span class="token number">6379</span> cluster addslots ,7120
redis-cli <span class="token parameter variable">-h</span> <span class="token number">192.168</span>.0.2 –p <span class="token number">6379</span> cluster addslots <span class="token number">7121,9945</span>
redis-cli <span class="token parameter variable">-h</span> <span class="token number">192.168</span>.0.3 –p <span class="token number">6379</span> cluster addslots <span class="token number">9946,13005</span>
redis-cli <span class="token parameter variable">-h</span> <span class="token number">192.168</span>.0.4 –p <span class="token number">6379</span> cluster addslots <span class="token number">13006,16383</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/DD2FAD94444B62E39476F3F726F209E8.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><ul><li>key <code>testkey_1</code> 和 <code>testkey_2</code> 经过 CRC16 计算后再对slots的总个数 16384 取模，结果分别匹配到了 cache1 和 cache3 上。</li></ul><h3 id="client访问数据集群的过程" tabindex="-1"><a class="header-anchor" href="#client访问数据集群的过程" aria-hidden="true">#</a> client访问数据集群的过程</h3><h4 id="一般情况" tabindex="-1"><a class="header-anchor" href="#一般情况" aria-hidden="true">#</a> 一般情况</h4><p>Redis 中的每个实例节点会<strong>将自己负责的哈希槽信息通过 Gossip 协议广播给集群中其他的实例</strong>，实现了slots分配信息的扩散</p><p>每个实例都知道整个集群的哈希槽分配情况以及映射信息。</p><p>所以客户端想要快捷的连接到服务端，并对某个redis数据进行快捷访问，一般是经过以下步骤：</p><ul><li>客户端<strong>连接任一实例</strong>，获取到slots与实例节点的映射关系，并<strong>将该映射关系的信息缓存在本地</strong>。</li><li><strong>将需要访问的redis信息的key，经过CRC16计算后，再对16384 取模得到对应的 Slot 索引</strong>。</li><li><strong>通过slot的位置进一步定位到具体所在的实例</strong>，再将请求发送到对应的实例上。</li></ul><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/7C39F35606B9A5E7C31941D9C6351D9C.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h4 id="特殊情况" tabindex="-1"><a class="header-anchor" href="#特殊情况" aria-hidden="true">#</a> 特殊情况</h4><p>在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：</p><ul><li>在集群中，实例有新增或删除，Redis 需要重新分配哈希槽；</li><li>为了负载均衡，Redis 需要把哈希槽在所有实例上重新分布一遍。</li></ul><p>实例之间可以通过相互传递消息，获得最新的哈希槽分配信息，客户端是无法主动感知这些变化的。这就会导致它<strong>缓存的分配信息和最新的分配信息就不一致</strong></p><p>Redis Cluster 方案提供了一种**重定向机制，**所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，客户端要再给一个新实例发送操作命令。</p><h5 id="moved" tabindex="-1"><a class="header-anchor" href="#moved" aria-hidden="true">#</a> MOVED</h5><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/350abedefcdbc39d6a8a8f1874eb0809-20221015223055-g6bfj2l.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>使用条件：<strong>slots的数据迁移已经完成</strong></p><p>效果：客户端请求key1，本地缓存中显示key1在实例A，但是请求实例A请求不到，实例A会给客户端返回一个MOVED命令</p><p>MOVED命令会告诉客户端应该去请求哪个实例能够请求到key1，并且<strong>MOVED命令还会更新本地的缓存</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>GET hello:key
<span class="token punctuation">(</span>error<span class="token punctuation">)</span> MOVED <span class="token number">13320</span> <span class="token number">172.16</span>.19.5:6379
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h5 id="ask" tabindex="-1"><a class="header-anchor" href="#ask" aria-hidden="true">#</a> ASK</h5><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/e93ae7f4edf30724d58bf68yy714eeb0-20221015223055-wlh553i.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>使用条件：<strong>slots的数据迁移未完成</strong></p><p>效果：客户端请求key1，本地缓存中显示key1在实例A，但是请求实例A请求不到，实例A会给客户端返回一个ASK命令</p><p>ASK 命令表示两层含义</p><ul><li>第一，表明 Slot 数据还在迁移中；</li><li>第二，ASK 命令把客户端所请求数据的最新实例地址返回给客户端</li></ul><p>客户端需要给实例 3 发送 ASKING 命令，然后再发送操作命令。ASKING命令的意思是：让这个实例允许执行客户端接下来发送的命令。然后，客户端再向这个实例发送 GET 命令，以读取数据。</p><p>和 MOVED 命令不同，<strong>ASK 命令并不会更新客户端缓存的哈希槽分配信息</strong></p><div class="language-bash line-numbers-mode" data-ext="sh"><pre class="language-bash"><code>GET hello:key
<span class="token punctuation">(</span>error<span class="token punctuation">)</span> ASK <span class="token number">13320</span> <span class="token number">172.16</span>.19.5:6379
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>MOVED命令和ASK命令的输出是一样的，都是键值对所在哈希槽位置和对应实例的IP地址，但是MOVED会更改本地缓存，ASK不会</p><p><strong>ASK 命令的作用只是让客户端能给新实例发送一次请求，而不像 MOVED 命令那样，会更改本地缓存，让后续所有命令都发往新实例。</strong></p><h4 id="客户端根据key查询value的步骤" tabindex="-1"><a class="header-anchor" href="#客户端根据key查询value的步骤" aria-hidden="true">#</a> 客户端根据key查询value的步骤</h4><ol><li>根据key通过CRC16运算后对16384 取模得到该key是存在哪一个slot里面</li><li>通过本地缓存中slot和Redis实例的映射关系，访问对应的实例，若缓存中没有，则随机访问一个实例</li><li>访问到如果该实例中没用对应的slot，则说明本地缓存与实例的缓存不一致，此时实例会返回客户端一个重定向报错信息 <ul><li>若该实例目前正在进行数据迁移，并且查询的key落在迁移的slot中 <ol><li>实例会返回客户端一个ASK报错信息</li><li>客户端收到ASK命令后，首先会向重定向的实例发去一个ASKING请求，让这个实例允许执行客户端接下来发送的命令</li><li>然后，客户端再向这个实例发送 GET 命令读取数据</li></ol></li><li>若该实例目前没有进行数据迁移 <ol><li>实例会返回客户端一个MOVED报错信息</li><li>客户端收到MOVED命令后，首先会修改本地缓存中对应的哈希槽与实例的映射关系</li><li>然后客户端再向这个实例发送GET命令读取数据</li></ol></li></ul></li><li>到达正确的Redis实例后再根据key去redisDb的数据字典中找值</li></ol><h3 id="数据复制过程和故障转移" tabindex="-1"><a class="header-anchor" href="#数据复制过程和故障转移" aria-hidden="true">#</a> 数据复制过程和故障转移</h3><h4 id="数据复制" tabindex="-1"><a class="header-anchor" href="#数据复制" aria-hidden="true">#</a> 数据复制</h4><p>分片集群模式也有主从节点，每个Master至少需要一个Slave节点，Slave 节点是通过主从复制方式同步主节点数据。</p><p>节点之间保持TCP通信，当Master发生了宕机， Redis Cluster自动会将对应的Slave节点选为Master，来继续提供服务。</p><p>与纯主从模式不同的是，主从节点之间并没有读写分离， <strong>Slave 只用作 Master 宕机的高可用备份，所以更合理来说应该是主备模式</strong>。</p><ul><li>如果主节点没有从节点，那么一旦发生故障时，集群将完全处于不可用状态</li><li>允许配置 <code>cluster-require-full-coverage</code>参数，即使部分节点不可用，其他节点正常提供服务，这是为了避免全盘宕机。 <ul><li>相当于丢失了一连串slots的数据</li></ul></li></ul><p>主从切换之后，故障恢复的主节点，会转化成新主节点的从节点。这种自愈模式对提高可用性非常有帮助。</p><h4 id="故障检测" tabindex="-1"><a class="header-anchor" href="#故障检测" aria-hidden="true">#</a> 故障检测</h4><p>故障检测是主节点之间相互检测</p><p>一个节点认为某个节点宕机不能说明这个节点真的挂起了，无法提供服务了。只有占据多数的实例节点都认为某个节点挂起了，这时候cluster才进行下线和主从切换的工作。</p><p>Redis 集群的节点采用 Gossip 协议来广播信息，每个节点都会定期向其他节点发送ping命令，如果接受ping消息的节点在指定时间内没有回复pong，则会认为该节点失联了（PFail），则发送ping的节点就把接受ping的节点标记为主观下线。</p><blockquote><p>分片集群的节点们会维护一张下线报告列表(Fail Report)，这张列表里面包含了所有节点的下线报告</p><p>例如<code>Node3:Node1,Node2</code>，表示节点1和节点2认为节点3主观下线</p><ol><li><p>首先每个节点都会自主判断其他节点状态，当一个节点A与自己断连时间过长，则判定该节点为PFAIL状态，并将A节点的判定状态随机Gossip给集群的其他节点。</p></li><li><p>其他节点接收到消息后，会累计节点A的下线报告数，如果节点A的下线报告数目超过了cluster_size/2，即说明一半以上的节点都认为A下线，那么就判定A真正的下线，标记为FAIL。</p></li><li><p>判定结束后，向集群广播节点A下线消息，其他节点都会更新自己维护的节点A的状态信息，标记A为FAIL。</p></li></ol></blockquote><p>如果集群半数以上的主节点都将主节点 xxx 标记为主观下线，则节点 xxx 将被标记为客观下线，然后向整个集群广播，让其它节点也知道该节点已经下线，并立即对下线的节点进行主从切换。</p><h4 id="主从故障转移-1" tabindex="-1"><a class="header-anchor" href="#主从故障转移-1" aria-hidden="true">#</a> 主从故障转移</h4><p>当从节点发现自己正在复制的主节点已下线，就开始对下线的主节点进行故障转移</p><ul><li>如果只有一个slave节点，则从节点会执行<code>SLAVEOF no one</code>命令，成为新的主节点</li><li>如果是多个slave节点，则采用选举模式进行，竞选出新的Master</li></ul><blockquote><p><strong>资格检查</strong></p></blockquote><ol><li>从节点接收到主节点宕机的广播后，会根据其最后一次跟主节点通信的时间来排名，越短排名越高，如果太久就不具备竞选的资格</li><li>根据这个排名，系统会让这些从节点统一休眠，休眠时间为500ms+由排名决定的附加时间，排名越高附加时间越短</li></ol><blockquote><p><strong>Master大选流程</strong>（跟哨兵leader选举流程相似）</p></blockquote><ol><li>集群中设立一个自增计数器，初始值为 0 ，每次执行故障转移选举，计数就会+1。</li><li>苏醒后的从节点会向集群所有master广播一条<code>CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST</code>消息，所有收到消息、并具备投票权的主节点都向这个从节点投票。</li><li>一个主节点只能投一票</li><li>其他在线的主节点们会把票投给第一个收到的投票命令，返回<code>CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK</code>消息，表示支持。</li><li>参与选举的从节点都会接收<code>CLUSTERMSG_TYPE_FAILOVER_AUTH_ACK</code>消息，如果收集到的选票 大于等于 $(n/2) + 1$ 支持，$n$代表所有具备选举权的Master，那么这个从节点就被选举为新主节点。</li><li>如果这一轮从节点都没能争取到足够多的票数，则发起再一轮选举（自增计数器+1），直至选出新的master。</li></ol><blockquote><p><strong>新旧主节点转移</strong></p></blockquote><ol><li>新的主节点会撤销所有对已下线主节点的slots指派，并将这些slots全部指派给自己。</li><li>新的主节点向集群广播一条PONG消息，这条PONG消息可以让集群中的其他节点立即知道这个节点已经由从节点变成了主节点，并且这个主节点已经接管了原本由已下线节点负责处理的槽。</li><li>新的主节点开始接收和自己负责处理的槽有关的命令请求，故障转移完成。</li></ol><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/354B704C05DFB2E65C15A9EFA351C74D.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h3 id="分片集群的架构" tabindex="-1"><a class="header-anchor" href="#分片集群的架构" aria-hidden="true">#</a> 分片集群的架构</h3><p>CAP理论：强一致性（Consistency），可用性（Availability）和分区容错性（Partition Tolerance）</p><p>Cluster作为一个分布式系统分区容错性一定是需要考虑的，因此P一定是有的</p><p>但有一点需要注意，分区容错性是允许某部分或者一些节点或者数据丢失的情况下，系统仍能继续工作。</p><p>这个一些取决于分布式系统里面各自的算法，常见的共识算法（这里的共识算法我猜是那种投票的）。</p><p>至于C和A则根据场景来的</p><ul><li>CP更强调数据的一致性，如果有节点挂掉，则所有节点返回失败的信息。</li><li>AP更强调服务的可用性，如果有节点挂掉，则节点返回自身写入的最新信息。</li></ul><p>RedisCluster是非CP的，主从复制是异步的，所以不可避免的会出现主从节点数据不一致的情况</p><p>RedisCluster有可能是AP的</p><ul><li><code>cluster-require-full-coverage</code>参数可以控制在部分节点不可用的情况下，其他节点能否请正常提供服务</li><li>配置了该参数，RedisCluster就是AP的，因为即使部分节点挂了，也可以保证Redis服务可用</li><li>如果没配，只要挂了一个节点，Redis服务就挂了，那也就没用可用性了，所以就不是AP的</li></ul><h3 id="rediscluster构建缓存集群的最佳实践" tabindex="-1"><a class="header-anchor" href="#rediscluster构建缓存集群的最佳实践" aria-hidden="true">#</a> RedisCluster构建缓存集群的最佳实践</h3><p><strong>所有存储系统都通用的实践方法：</strong></p><ul><li>数据量太大查询慢怎么办？存档历史数据或者分库分表，这是 <strong><u>数据分片</u></strong>。</li><li>并发太高扛不住怎么办？读写分离，这是 <u><strong>增加实例数</strong></u>。</li><li>数据库宕机怎么办？增加从节点，主节点宕机的时候用从节点顶上，这是 <strong><u>主从复制</u></strong>。但是这里面要特别注意数据一致性的问题。</li></ul><h4 id="解决数据量大-高可用和高并发" tabindex="-1"><a class="header-anchor" href="#解决数据量大-高可用和高并发" aria-hidden="true">#</a> 解决数据量大，高可用和高并发</h4><p>Redis Cluster比单个节点的Redis，能存更多的数据，并且可以做到高可用，在单个节点故障的情况下，可以继续提供服务</p><h5 id="数据分片" tabindex="-1"><a class="header-anchor" href="#数据分片" aria-hidden="true">#</a> 数据分片</h5><blockquote><p><strong>保存更多数据</strong></p></blockquote><p>使用哈希槽对所有数据进行分片：一个节点控制一部分的哈希槽，命令通过key查询到对应存储的哈希槽，然后定位到控制这个哈希槽的节点，就能找到该数据了</p><p>Redis Cluster 就 <strong>可以通过水平扩容来增加集群的存储容量</strong></p><ul><li>每次往集群增加节点的时候，需要从集群的那些老节点中，搬运一些槽到新节点</li><li>可以手动指定哪些槽迁移到新节点上，也可以利用官方提供的 <a href="https://redis.io/topics/cluster-tutorial#creating-the-cluster" target="_blank" rel="noopener noreferrer">redis-trib.rb<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>脚本来自动重新分配槽，自动迁移。</li></ul><p><strong><u>分片可以解决 Redis 保存海量数据的问题，并且客观上提升了 Redis 的并发能力和查询性能</u></strong>。</p><p>但是并不能解决高可用的问题，每个节点都保存了整个集群数据的一个子集，任何一个节点宕机，都会导致这个宕机节点上的那部分数据无法访问。</p><h5 id="主从复制" tabindex="-1"><a class="header-anchor" href="#主从复制" aria-hidden="true">#</a> 主从复制</h5><blockquote><p><strong>解决高可用</strong></p></blockquote><p>通过增加从节点，做主从复制来提高可用性</p><p>Redis Cluster可以给每个主节点添加多个从节点来做数据备份</p><p>一旦主节点宕机，可以在从节点中选举一个出来升级为主节点，防止主节点宕机后部分数据无法访问</p><blockquote><p><strong>解决高并发</strong></p></blockquote><p>一般来说，Redis Cluster 进行了分片之后，每个分片都会承接一部分并发的请求，加上 Redis 本身单节点的性能就非常高，所以<strong>大部分情况下不需要再像 MySQL 那样做读写分离来解决高并发的问题</strong>。</p><p>默认情况下，集群的读写请求都是由主节点负责的，从节点只是起一个热备的作用。当然了，Redis Cluster 也支持读写分离，在从节点上读取数据。</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/14238e20a7fd63760a3261d87ffa6583.14238e20.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h4 id="redis-cluster不适合超大规模集群" tabindex="-1"><a class="header-anchor" href="#redis-cluster不适合超大规模集群" aria-hidden="true">#</a> Redis Cluster不适合超大规模集群</h4><p>Redis Cluster 的优点是易于使用。分片、主从复制、弹性扩容这些功能都可以做到自动化，通过简单的部署就可以获得一个大容量、高可靠、高可用的 Redis 集群，并且对于应用来说，近乎于是透明的。</p><p>所以，<strong>Redis Cluster 是非常适合构建中小规模 Redis 集群</strong>，这里的中小规模指的是，大概几个到几十个节点这样规模的 Redis 集群。</p><p><strong>但是 Redis Cluster 不太适合构建超大规模集群，主要原因是，它采用了去中心化的设计。</strong></p><p>Redis 的每个节点上，都保存了所有槽和节点的映射关系表，客户端可以访问任意一个节点，再通过重定向命令，找到数据所在的那个节点。这个映射关系表，它是如何更新的呢？比如说，集群加入了新节点，或者某个主节点宕机了，新的主节点被选举出来，这些情况下，都需要更新集群每一个节点上的映射关系表。</p><p>Redis Cluster 采用了一种去中心化的 <strong>流言 (Gossip) 协议</strong> 来传播集群配置的变化。</p><p>所谓流言，就是八卦，比如说，我们上学的时候，班上谁和谁偷偷好上了，搞对象，那用不了一天，全班同学都知道了。咋知道的？张三看见了，告诉李四，李四和王小二特别好，又告诉了王小二，这样人传人，不久就传遍全班了。这个就是八卦协议的传播原理。</p><p>这个八卦协议它的好处是去中心化，传八卦不需要组织，吃瓜群众自发就传开了。这样部署和维护就更简单，也能避免中心节点的单点故障。<strong>八卦协议的缺点就是传播速度慢，并且是集群规模越大，传播的越慢</strong>。这个也很好理解，比如说，换成某两个特别出名的明星搞对象，即使是全国人民都很八卦，但要想让全国每一个人都知道这个消息，还是需要很长的时间。在集群规模太大的情况下，数据不同步的问题会被明显放大，还有一定的不确定性，如果出现问题很难排查。</p><h4 id="如何用redis构建超大规模集群" tabindex="-1"><a class="header-anchor" href="#如何用redis构建超大规模集群" aria-hidden="true">#</a> 如何用Redis构建超大规模集群</h4><p>Redis Cluster 不太适合用于大规模集群，所以很多大厂，都选择自己去搭建 Redis 集群。每一家的解决方案都有自己的特色，但其实总体的架构都是大同小异的。</p><h5 id="基于代理的方式" tabindex="-1"><a class="header-anchor" href="#基于代理的方式" aria-hidden="true">#</a> 基于代理的方式</h5><p><strong>在客户端和 Redis 节点之间，还需要增加一层代理服务</strong></p><p>这个代理服务有三个作用:</p><ol><li>第一个作用是，<strong>负责在客户端和 Redis 节点之间转发请求和响应</strong>。 <ul><li>客户端只和代理服务打交道</li><li>代理收到客户端的请求之后，再转发到对应的 Redis 节点上，节点返回的响应再经由代理转发返回给客户端。</li></ul></li><li>第二个作用是，<strong>负责监控集群中所有 Redis 节点状态</strong>，如果发现有问题节点，及时进行主从切换。</li><li>第三个作用就是，<strong>维护集群的元数据</strong><ul><li>元数据主要就是集群所有节点的主从信息，以及槽和节点关系映射表</li></ul></li></ol><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/string.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>开源的 Redis 集群方案 twemproxy 和 Codis，都是这种架构的。</p><p><strong>优点：对客户端透明</strong></p><p>在客户端视角来看，<strong>整个集群和一个超大容量的单节点 Redis 是一样的</strong></p><p>并且，由于分片算法是代理服务控制的，扩容也比较方便，新节点加入集群后，直接修改代理服务中的元数据就可以完成扩容。</p><p><strong>缺点：增加了一层代理转发，每次数据访问的链路更长了，会带来一定的性能损失</strong>。</p><p>而且，代理服务本身又是集群的一个单点，当然，我们可以把代理服务也做成一个集群来解决单点问题，那样集群就更复杂了。</p><h5 id="将代理的功能放进客户端" tabindex="-1"><a class="header-anchor" href="#将代理的功能放进客户端" aria-hidden="true">#</a> 将代理的功能放进客户端</h5><p>不用这个代理服务，把代理服务的寻址功能前移到客户端中去。</p><p>客户端在发起请求之前，先去查询元数据，就可以知道要访问的是哪个分片和哪个节点，然后直连对应的 Redis 节点访问数据</p><blockquote><p>JAVA 客户端 jedis 就支持这个功能</p></blockquote><p>客户端不用每次都去查询元数据，因为这个元数据是不怎么变化的</p><p>客户端可以自己缓存元数据，这样访问性能基本上和单机版的 Redis 是一样的。如果某个分片的主节点宕机了，新的主节点被选举出来之后，更新元数据里面的信息。对集群的扩容操作也比较简单，除了迁移数据的工作必须要做以外，更新一下元数据就可以了</p><figure><img src="http://lgy-markdown-img.oss-cn-guangzhou.aliyuncs.com/image/dcaced0a9ce9842ef688c9626accdcda.dcaced0a.jpg" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>虽然说，这个元数据服务仍然是一个单点，但是它的数据量不大，访问量也不大，相对就比较容易实现。</p><ul><li>可以用 ZooKeeper、etcd 甚至 MySQL 都能满足要求。</li></ul><p><strong>这个方案应该是最适合超大规模 Redis 集群的方案了，在性能、弹性、高可用几方面表现都非常好</strong></p><p>缺点是整个架构比较复杂，客户端不能通用，需要开发定制化的 Redis 客户端，只有规模足够大的企业才负担得起。</p><hr></div><!--[--><!----><!--]--><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lgy8888/my_blog.git/edit/main/src/backEnd/Redis/Redis高可用.md" rel="noopener noreferrer" target="_blank" aria-label="在 GitHub 上编辑此页" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewBox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->在 GitHub 上编辑此页<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item git-info"><!----><!----></div></footer><nav class="vp-page-nav"><a aria-label="Redis持久化面试题汇总" class="vp-link nav-link prev nav-link prev" href="/my_blog/backEnd/Redis/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><div class="hint"><span class="arrow start"></span>上一页</div><div class="link"><span class="font-icon icon iconfont icon-write" style=""></span>Redis持久化面试题汇总</div></a><a aria-label="Redis高可用面试题汇总" class="vp-link nav-link next nav-link next" href="/my_blog/backEnd/Redis/Redis%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9D%A2%E8%AF%95%E9%A2%98%E6%B1%87%E6%80%BB.html"><div class="hint">下一页<span class="arrow end"></span></div><div class="link">Redis高可用面试题汇总<span class="font-icon icon iconfont icon-write" style=""></span></div></a></nav><div id="comment" class="giscus-wrapper input-top" style="display:block;"><div class="loading-icon-wrapper" style="display:flex;align-items:center;justify-content:center;height:96px"><svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" preserveAspectRatio="xMidYMid" viewBox="25 25 50 50"><animateTransform attributeName="transform" type="rotate" dur="2s" keyTimes="0;1" repeatCount="indefinite" values="0;360"></animateTransform><circle cx="50" cy="50" r="20" fill="none" stroke="currentColor" stroke-width="4" stroke-linecap="round"><animate attributeName="stroke-dasharray" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="1,200;90,200;1,200"></animate><animate attributeName="stroke-dashoffset" dur="1.5s" keyTimes="0;0.5;1" repeatCount="indefinite" values="0;-35px;-125px"></animate></circle></svg></div></div><!--[--><!----><!--]--><!--]--></main><!--]--><footer class="vp-footer-wrapper"><div class="vp-footer">林光远的个人笔记网站</div><div class="vp-copyright">Copyright © 2024 LGYNB</div></footer></div><!--]--><!----><!--]--></div>
    <script type="module" src="/my_blog/assets/app-OTaO6_y0.js" defer></script>
  </body>
</html>
