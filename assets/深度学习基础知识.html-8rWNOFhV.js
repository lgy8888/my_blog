import{_ as e}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as l,c as d,f as i}from"./app-OTaO6_y0.js";const o={},a=i('<h1 id="深度学习基础知识" tabindex="-1"><a class="header-anchor" href="#深度学习基础知识" aria-hidden="true">#</a> 深度学习基础知识</h1><h2 id="梯度下降算法" tabindex="-1"><a class="header-anchor" href="#梯度下降算法" aria-hidden="true">#</a> 梯度下降算法</h2><ul><li>梯度下降法是一种致力于找到函数极值点的算法</li><li>梯度输出的向量表明了在每个位置损失函数增长最快的方向</li><li>梯度下降是沿着梯度表明方向的反方向移动</li></ul><h2 id="学习速率" tabindex="-1"><a class="header-anchor" href="#学习速率" aria-hidden="true">#</a> 学习速率</h2><ul><li><p>梯度就是表明<strong>损失函数相对参数的变化率</strong></p></li><li><p><strong>对梯度进行缩放</strong>的参数被称为学习速率（learning rate）</p></li><li><p>需要为<code>lr</code>指定正确的值</p><ul><li>若<code>lr</code>太小，则找到损失函数极小值点时可能需要许多论迭代</li><li>若<code>lr</code>太大，则算法可能会跳过极小值点并进行周期性的跳跃</li></ul></li><li><p>学习速率同样可以防止学习的步幅太大导致在少数的epoch中到达极值点，而无法顾全大局</p></li><li><p>实践中可以通过查看损失函数值随时间的变化曲线来判断学习速率的选取是否合适</p></li></ul><h2 id="局部极值点" tabindex="-1"><a class="header-anchor" href="#局部极值点" aria-hidden="true">#</a> 局部极值点</h2><ul><li>通过将权值随机初始化来改善局部极值的我呢提</li><li>权值的初值使用随机值可以增加从靠近全局最优点附近开始下降的机会</li></ul><h2 id="反向传播算法" tabindex="-1"><a class="header-anchor" href="#反向传播算法" aria-hidden="true">#</a> 反向传播算法</h2><ul><li><p>反向传播算法是一种高效计算数据流图中梯度的技术</p></li><li><p>每一层的导数都是后一层的导数与前一层输出之积</p></li><li><p>步骤</p><ul><li>正向（forward）：从输入开始，逐一计算每个隐含层的输出，直到输出层</li><li>反向（backward）：开始计算导数，从输出层经过各个隐含层逐一反向传播，为减少计算量，需要对已完成计算的元素进行复用</li></ul></li></ul><h2 id="优化器" tabindex="-1"><a class="header-anchor" href="#优化器" aria-hidden="true">#</a> 优化器</h2><ul><li>优化器（optimizer）：是根据反向传播计算出的梯度，优化模型参数的内置方法</li></ul><h3 id="sgd" tabindex="-1"><a class="header-anchor" href="#sgd" aria-hidden="true">#</a> SGD</h3><ul><li><p><code>SGD</code>（随机梯度下降优化器）：<code>SGD</code>和<code>min-batch</code>是同个意思，抽取m个小批量（独立同分布）样本，通过计算他们的梯度均值来优化参数</p></li><li><p><code>SGD</code>参数</p><ul><li><code>lr</code>：<code>float &gt;= 0.</code>，学习率</li><li><code>momentum</code>：<code>float &gt;= 0.</code>，用于加速SGD在相关方向上前进，并抑制震荡</li><li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值</li><li><code>nesterov</code>：<code>boolean</code>，是否使用<code>Nesterov</code>动量</li></ul></li></ul><h3 id="rmsprop" tabindex="-1"><a class="header-anchor" href="#rmsprop" aria-hidden="true">#</a> RMSProp</h3><ul><li><p>经验上，<code>RMSProp</code>被证明有效且实用的深度学习网络优化算法</p></li><li><p><code>RMSProp</code>增加了一个衰减系数来控制历史信息的获取多少</p></li><li><p><code>RMSProp</code>会对学习率进行衰减</p></li><li><p>通常是训练循环神经网络（RNN）的不错选择</p></li><li><p>建议使用优化器的默认参数（只自定义<code>lr</code>）</p></li></ul><h3 id="adam" tabindex="-1"><a class="header-anchor" href="#adam" aria-hidden="true">#</a> Adam</h3><ul><li><p><code>Adam</code>算法是可以看做是修正后的<code>Momentum + RMSProp</code>算法</p></li><li><p><code>Adam</code>通过通常被认为对超参数的选择相当鲁棒（不敏感）</p></li><li><p>学习率建议为0.001</p></li><li><p><code>Adam</code>是一种可以替代传统随机梯度下降过程的一阶优化算法，他能基于训练数据迭代地更新神经网络权重</p></li><li><p><code>Adam</code>通过计算梯度的一阶矩估计（一阶导数）和二阶矩估计（二阶导数）而为不同的参数设计独立的自适应性学习率</p></li><li><p><code>Adam</code>参数</p><ul><li><code>lr</code>：<code>float &gt;= 0.</code>，学习率</li><li><code>beta_1</code>：<code>float, 0&lt; beta &lt; 1</code>，通常接近于1，一阶矩估计</li><li><code>beta_2</code>：<code>float, 0&lt; beta &lt; 1</code>，通常接近于1，二阶矩估计</li><li><code>decay</code>：<code>float &gt;= 0</code>，每次参数更新后学习率衰减值</li></ul></li></ul>',17),c=[a];function r(t,h){return l(),d("div",null,c)}const s=e(o,[["render",r],["__file","深度学习基础知识.html.vue"]]);export{s as default};
