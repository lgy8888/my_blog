import{_ as n}from"./plugin-vue_export-helper-x3n3nnut.js";import{o as s,c as a,f as p}from"./app-OTaO6_y0.js";const t={},o=p(`<h1 id="pytorch入门实例" tabindex="-1"><a class="header-anchor" href="#pytorch入门实例" aria-hidden="true">#</a> pytorch入门实例</h1><h2 id="数据预处理" tabindex="-1"><a class="header-anchor" href="#数据预处理" aria-hidden="true">#</a> 数据预处理</h2><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 用pandas读取csv文件里面的数据</span>
data <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span><span class="token string">&#39;./1.基础部分(第1-7章)参考代码和数据集/第2章/Income1.csv&#39;</span><span class="token punctuation">)</span>
<span class="token comment"># 线性回归的两个因素：自变量X和因变量Y</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">.</span>Education<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>data<span class="token punctuation">.</span>Income<span class="token punctuation">.</span>values<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="模型初始化" tabindex="-1"><a class="header-anchor" href="#模型初始化" aria-hidden="true">#</a> 模型初始化</h2><p><strong>pytorch</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token comment"># 线性回归模型类（继承自nn中的Module库）</span>
<span class="token keyword">class</span> <span class="token class-name">EIModel</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EIModel<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
	<span class="token comment"># 实现模型的前向传播逻辑</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits<span class="token punctuation">;</span>

model <span class="token operator">=</span> EIModel<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 模型初始化</span>
loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数初始化</span>
opt <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">)</span> <span class="token comment"># 优化算法初始化</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>分步</strong></p><ul><li>模型的公式：$w@x + b$</li></ul><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code>w <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 初始化权重为符合正态分布的随机数</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment"># 初始化偏置为0</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.0001</span> <span class="token comment"># 初始化超参数（学习率）为0.0001</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>模型、损失函数和优化函数的初始化在训练过程中</li></ul><h2 id="训练模型" tabindex="-1"><a class="header-anchor" href="#训练模型" aria-hidden="true">#</a> 训练模型</h2><p><strong>pytorch</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 对全部数据进行训练5000次</span>
    <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> Y<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 对x和y同时进行训练</span>
        y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 将x输入模型获取y的预测值y_pred</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>y_pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token comment"># 用y_pred和y来计算损失</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 梯度清零（pytorch的梯度会累积）</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 根据损失loss来计算梯度（反向传播）</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 优化参数（向梯度下降的方向移动）</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><strong>分步</strong></p><div class="language-python line-numbers-mode" data-ext="py"><pre class="language-python"><code><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> x<span class="token punctuation">,</span>y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span>Y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        y_pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span>w<span class="token punctuation">)</span> <span class="token operator">+</span> b <span class="token comment"># 构建模型</span>
        loss <span class="token operator">=</span> <span class="token punctuation">(</span>y <span class="token operator">-</span> y_pred<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 损失函数（均方差 -&gt; nn.MSELoss()）</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> w<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 权重梯度清零 相当于opt.zero_grad()</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> b<span class="token punctuation">.</span>grad <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            b<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 偏置梯度清零 相当于opt.zero_grad()</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 等价于opt（torch.optim.SGD(model.parameters(), lr=0.0001)）</span>
            w<span class="token punctuation">.</span>data <span class="token operator">-=</span> w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data <span class="token operator">*</span> learning_rate
            b<span class="token punctuation">.</span>data <span class="token operator">-=</span> b<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data <span class="token operator">*</span> learning_rate
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><h3 id="pytorch" tabindex="-1"><a class="header-anchor" href="#pytorch" aria-hidden="true">#</a> pytorch</h3><ul><li><p>定义一个<strong>模型类</strong>，继承<code>nn.Module</code></p></li><li><p>在<code>__init__</code>方法中，通过调用<code>super(Model, self).__init__()</code>来初始化父类<code>nn.Module</code>，确保正确地初始化模型。</p></li><li><p><code>self.linear = nn.Linear(in_features=1, out_features=1)</code>定义了一个线性层，其中<code>in_features=1</code>表示输入特征数为1，<code>out_features=1</code>表示输出特征数为1。这意味着该模型将接收一个1维的输入，并输出一个1维的结果。</p></li><li><p><code>forward</code>方法是实现模型的前向传播逻辑。在这个方法中，输入数据<code>inputs</code>通过<code>self.linear</code>线性层进行处理，然后返回线性层的输出<code>logits</code>作为模型的预测结果。</p></li><li><p><strong>损失函数</strong>使用<code>nn</code>中提供的构造方法</p></li><li><p><strong>优化算法</strong>使用<code>torch.optim</code>中提供的构造方法</p></li><li><p>训练时，按照 <strong>获取预测值 -&gt; 获取损失值 -&gt; 梯度清零 -&gt; 计算梯度(反向传播) -&gt; 优化参数</strong> 的顺序依次调取对应方法以及传参</p></li></ul><h3 id="分步" tabindex="-1"><a class="header-anchor" href="#分步" aria-hidden="true">#</a> 分步</h3><ul><li><strong>模型</strong>使用公式定义，参数使用随机张量定义</li><li><strong>损失函数</strong>使用公式定义</li><li>训练时需判断参数的梯度是否为零，非零则清空</li><li>优化参数时不需要记录梯度</li><li>梯度下降具体表现为：参数减掉对应的梯度*学习率</li></ul>`,20),e=[o];function c(l,i){return s(),a("div",null,e)}const d=n(t,[["render",c],["__file","pytorch入门实例.html.vue"]]);export{d as default};
