const e=JSON.parse('{"key":"v-48f774cc","path":"/ai/pytorch/%20%E6%96%87%E6%9C%AC%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B.html","title":"文本与注意力","lang":"zh-CN","frontmatter":{"description":"RNN其他应用\\r关键字提取（many to one）; \\r将一段话作为序列输入网络，输出只取最后一个状态，用它来表示这句话的关键字; \\r手写数字识别（many to many）; \\r输入与输出都是序列，一般是定长的; \\r模型设计上，利用RNN单元在每个时间步上的输出得到序列结果; \\rseq2seq; \\r一般是机器翻译的任务中出现，将一句中文翻译成英文，...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/my_blog/ai/pytorch/%20%E6%96%87%E6%9C%AC%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B.html"}],["meta",{"property":"og:site_name","content":"林光远的个人笔记"}],["meta",{"property":"og:title","content":"文本与注意力"}],["meta",{"property":"og:description","content":"RNN其他应用\\r关键字提取（many to one）; \\r将一段话作为序列输入网络，输出只取最后一个状态，用它来表示这句话的关键字; \\r手写数字识别（many to many）; \\r输入与输出都是序列，一般是定长的; \\r模型设计上，利用RNN单元在每个时间步上的输出得到序列结果; \\rseq2seq; \\r一般是机器翻译的任务中出现，将一句中文翻译成英文，..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"article:author","content":"LGYNB"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"文本与注意力\\",\\"image\\":[\\"\\"],\\"dateModified\\":null,\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"LGYNB\\",\\"url\\":\\"/\\"}]}"]]},"headers":[{"level":2,"title":"RNN其他应用","slug":"rnn其他应用","link":"#rnn其他应用","children":[]},{"level":2,"title":"Seq2Seq","slug":"seq2seq","link":"#seq2seq","children":[{"level":3,"title":"Seq2Seq的构成","slug":"seq2seq的构成","link":"#seq2seq的构成","children":[]},{"level":3,"title":"Seq2Seq结构","slug":"seq2seq结构","link":"#seq2seq结构","children":[]},{"level":3,"title":"Seq2Seq任务流程","slug":"seq2seq任务流程","link":"#seq2seq任务流程","children":[]},{"level":3,"title":"Seq2Seq缺陷","slug":"seq2seq缺陷","link":"#seq2seq缺陷","children":[]}]},{"level":2,"title":"注意力机制","slug":"注意力机制","link":"#注意力机制","children":[{"level":3,"title":"注意力机制原理","slug":"注意力机制原理","link":"#注意力机制原理","children":[]},{"level":3,"title":"使用注意力机制的Seq2Seq","slug":"使用注意力机制的seq2seq","link":"#使用注意力机制的seq2seq","children":[]}]},{"level":2,"title":"自注意力机制","slug":"自注意力机制","link":"#自注意力机制","children":[{"level":3,"title":"循环神经网络的问题","slug":"循环神经网络的问题","link":"#循环神经网络的问题","children":[]},{"level":3,"title":"自注意力机制","slug":"自注意力机制-1","link":"#自注意力机制-1","children":[]},{"level":3,"title":"自注意力机制原理","slug":"自注意力机制原理","link":"#自注意力机制原理","children":[]}]},{"level":2,"title":"Transformer模型","slug":"transformer模型","link":"#transformer模型","children":[{"level":3,"title":"Transformer模型结构","slug":"transformer模型结构","link":"#transformer模型结构","children":[]},{"level":3,"title":"Pytorch中自注意力机制模块","slug":"pytorch中自注意力机制模块","link":"#pytorch中自注意力机制模块","children":[]}]}],"git":{},"readingTime":{"minutes":10.8,"words":3241},"filePathRelative":"ai/pytorch/ 文本与注意力.md","autoDesc":true}');export{e as data};
